<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>PADDLE</title>
    <link href="https://paddle-cluster.github.io/feed.xml" rel="self" />
    <link href="https://paddle-cluster.github.io" />
    <updated>2020-02-15T20:00:50+01:00</updated>
    <author>
        <name>Thomas Hillman</name>
    </author>
    <id>https://paddle-cluster.github.io</id>

    <entry>
        <title>Transana quick start</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/transana-quick-start.html"/>
        <id>https://paddle-cluster.github.io/transana-quick-start.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T19:03:23+01:00</updated>
            <summary>
                <![CDATA[
                    Transana is an application developed by David Woods which can run on either Mac OS X or Windows and allows users to perform qualitative analysis of video data. In many ways it is similar to other QDA software like Atlas.ti or Nvivo but with much&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><a href="https://www.transana.com/" rel="nofollow">Transana</a> is an application developed by David Woods which can run on either Mac OS X or Windows and allows users to perform qualitative analysis of video data. In many ways it is similar to other QDA software like Atlas.ti or Nvivo but with much better features for handling video data such as interviews and observations.</p>
<p>The really useful basic feature of Transana is that videos are synchronized with their transcripts so that when an instance is coded in either the video or the transcript it is automatically connected to the other.</p>
<h2 id="mcetoc_1e150973o2">Tutorials</h2>
<p>This series of tutorials cover the basic concepts needed to start using Transana to do qualitative analysis of video (or audio) data.</p>
<ul>
<li><a href="https://vimeo.com/7433535" rel="nofollow">Getting started</a></li>
<li><a href="https://vimeo.com/7433616" rel="nofollow">Adding video</a></li>
<li><a href="https://vimeo.com/7433688" rel="nofollow">Creating or importing a transcript</a></li>
<li><a href="https://vimeo.com/7434003" rel="nofollow">Identifying instances</a></li>
<li><a href="https://vimeo.com/7434263" rel="nofollow">Defining keywords</a></li>
</ul>
<h2 id="mcetoc_1e1509a8s3">Useful Transana keycommands</h2>
<p>Ctrl on Windows/Cmd on Mac</p>
<ul>
<li>Ctrl/Cmd-D Pause / Play video</li>
<li>Ctrl/Cmd-S Pause video / Rewind video 2 seconds and play</li>
<li>Ctrl/Cmd-A Rewind video 10 seconds and play</li>
<li>Ctrl/Cmd-F Fastforward 10 seconds</li>
<li>Ctrl/Cmd-T Insert time point</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Video workflow</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/video-workflow.html"/>
        <id>https://paddle-cluster.github.io/video-workflow.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T19:02:15+01:00</updated>
            <summary>
                <![CDATA[
                    Starting at the point of having video recorded on a camera, my workflow tends to look something like this (though it depends a bit on what my research goal is):Use MPEG Streamclip to capture and compress the video from the cameraIf the video is of&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Starting at the point of having video recorded on a camera, my workflow tends to look something like this (though it depends a bit on what my research goal is):</p>
<ul>
<li>Use <a href="http://www.squared5.com/" rel="nofollow">MPEG Streamclip</a> to capture and compress the video from the camera</li>
<li>If the video is of an interview, transcribe it using <a href="http://www.bartastechnologies.com/products/transcriva/" rel="nofollow">Transcriva</a> (a similar program for Windows users is <a href="https://www.inqscribe.com/" rel="nofollow">Inqscribe</a>)
<ul>
<li>Export the transcript as an RTF file</li>
<li>Import the video (and transcript if its an interview) into <a href="https://www.transana.com/" rel="nofollow">Transana</a>.</li>
</ul>
</li>
<li>If the video is of observational in nature, create a content log (as a kind of fake transcript) with time coding in Transana. This or a detailed transcript allows clips to be identified and categorized</li>
<li>Usually at this point, I create keywords for structural characteristics such as which participant is speaking and the materials involved. I then go through the video and identify where these characteristics exist. This gives me a way to break down the video and if I have a lot of videos, it allows me to search across them for similar instances.</li>
<li>Using the clips and keywords functions in Transana, I identify and organize instances across the video data I have. I tend to work in an inductive recursive way and as I create new categories and keywords, I go back through the instances I have already identified.</li>
<li>Once I have identified specific short sections of a video or videos that I want to perform more detailed analysis on, I the move to <a href="https://tla.mpi.nl/tools/tla-tools/elan/" rel="nofollow">ELAN</a></li>
<li>In ELAN, I usually create multiple channels and input as much timecoded detail of the interactions in a clip as possible. The might include:
<ul>
<li>A channel for the discourse of each speaker</li>
<li>A channel for gestures</li>
<li>A channel for physical interactions with the technolog involved</li>
<li>A channel for the communicative actions made by the technology</li>
</ul>
</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Mapping with Gephi</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/mapping-publication-keywords-introduction-to-gephi.html"/>
        <id>https://paddle-cluster.github.io/mapping-publication-keywords-introduction-to-gephi.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T18:05:38+01:00</updated>
            <summary>
                <![CDATA[
                    This tutorial walks through the process of mapping the keywords for scientific publications. This is most useful for conducting systematic literature reviews, but the tutorial can also be used as a general introduction to mapping data using network graphing techniques. To follow this tutorial, you&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>This tutorial walks through the process of mapping the keywords for scientific publications. This is most useful for conducting systematic literature reviews, but the tutorial can also be used as a general introduction to mapping data using network graphing techniques.</p>
<p>To follow this tutorial, you will need to have the network graphing tool <a href="https://gephi.org/" rel="nofollow">Gephi</a> installed. Once you have installed Gephi, you will need to go to Tools &gt; Plugins and install the <a href="https://gephi.org/plugins/#/plugin/excel-csv-converter-to-network" rel="nofollow">Convert Excel and csv files to networks</a> plugin.</p>
<p>The tutorial uses data collected from <a href="http://www.scopus.com/" rel="nofollow">Scopus</a>. If you want to work with your own data, you will likely need institutional access to Scopus. Otherwise, you can <a href="https://github.com/paddle-cluster/constantmethod/blob/master/selectedPubsMetaData.csv">download the data</a> that I use in the tutorial instead.</p>
<h2 id="mcetoc_1e14s8pr13"><a id="user-content-getting-data-about-publications-from-scopus" class="anchor" aria-hidden="true" href="https://github.com/paddle-cluster/constantmethod/blob/master/mappingkeywords.md#getting-data-about-publications-from-scopus"></a>Getting data about publications from Scopus</h2>
<p>To get a list of keywords to work with, we will need to search for publications. This could be done on a variety of databases, but <a href="http://www.scopus.com/" rel="nofollow">Scopus</a> is one of the easiest to use.</p>
<p>Once you have searched for a relevant topic and assembled a list of relevant publications in Scopus, check the box next to All at the top of the list to select all the publications and click on CSV export. If you have more than 2000 publications in your list, Scopus will ask you if you want all the information for the first 2000 or just the citation information for up to 20,000. For our purposes, we only need the keywords that are part of the citation information so select that. If you click on the arrow next to CSV export, you can specify which specific information you want to export and it is interesting to look at what is available.</p>
<p>Once you have downloaded the CSV export or the <a href="https://github.com/constantmethod/constantmethod.github.io/blob/master/selectedPubsMetaData.csv">example data</a> I collected, you can move to Gephi.</p>
<h2 id="mcetoc_1e14s8tb74">Making a network from lists of data</h2>
<p>The CSV file assembled by Scopus contains a lot of different columns, but the one we are interested in is Index Keywords. These are the keywords assigned by Scopus to each publication, but you could also do this with the keywords that the authors have chosen.</p>
<p>In Gephi, go to File &gt; Import. Not Import spreadsheet... or Import Database, just Import. This should start the Import Wizard where you probably only have one wizard available. Make sure that under Category you have selected Data importer and under Wizards Type you have selected Convert Excel and csv files to networks.</p>
<p>Click on Next and select the CSV file from Scopus. This file is a 'Comma Seperated File' meaning that there is a comma between each data field. Make sure that comma is selected as the field delimiter and that 'file includes headers' is checked, and click Next.</p>
<p>You should now see a dialogue asking you to select agents. Agents are produced from the fields of a particular data type and you should see all the data types (or columns) from your CSV file listed. This is where the list gets turned into a network and to do that we need to connect individual data fields together. Since we are interested in the way keywords appear in our datset of publications, we will connect Index Keywords to Index Keywords. This means that a connection will be made between two publications when the same keyword appears for them. Click on Next.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_agents.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_agents.png?raw=true" alt="Import Wizard agents"></a></p>
<p>Each publication has more than one keyword so we need to tell Gephi how to read each keyword independently. In this case, the keywords are seperated by semicolons so make sure that delimiter is selected for subfields and click Next.</p>
<p>You will now be asked if you want to create a dynamic network. This means that each network connection would be associated with a time stamp, making it possible to visualize how the network changes over time. This can be done with the Scopus dataset, but just skip it for now by clicking on Next without selecting anything.</p>
<p>You should now see three options. Check 'create links...' to create the connections between publications with same keywords and click on 'remove self-loops' to filter out case of publications that might mistakenly have the same keyword more than once. Leave 'remove duplicates' unchecked as we are not just interested in the presence of connections, but also in the number of them. Click on Next and then on Finish on the next screen.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_selfloops.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_selfloops.png?raw=true" alt="Import Wizard agents"></a></p>
<p>An Import Report will pop up with some errors from missing data that don't really matter for our purposes. Click on OK to display your network.</p>
<h2 id="mcetoc_1e14s95a95">Laying out the network with Gephi</h2>
<p>Once you have created a network from the list of keywords, you will probably see a mess of dots and lines on the screen. We now need to layout the network in a way that reveals its properties.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_mess.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_mess.png?raw=true" alt="Import Wizard agents"></a></p>
<p>Start by going to the Layout window and choosing a Layout. There are many different layout algorithms for visualising networks, but for our purposes, ForceAtlas2 should work well. The particular algorithm you choose depends on what you want to visualize so its worth trying them out.</p>
<p>With ForceAtlas2 selected, click on Run. Once the network has started to stabilize and isn't moving very much click on Stop. The result should still look like a clumped up mess of dots, but you should see that certain clusters are starting to appear.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_forceatlas1.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_forceatlas1.png?raw=true" alt="Initial network visualisation"></a></p>
<p>ForceAtlas2 simulates gravitational and repulsion forces between the nodes (dots) in the network. The more connections between two nodes, the more gravational pull between them. To stop all the nodes from bunching together we can either adjust the Gravity or the Scaling. By default scaling is set at 2.0 and Gravity at 1.0. Try changing these values and running the algortihm to see what happens. In the end, I settled on leaving the gravity at 1.0 and changing the scaling to 50.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_forceatlas2.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_forceatlas2.png?raw=true" alt="Network layout"></a></p>
<p>Now we can scale the nodes to show which are the most frequent keywords and turn on labels. To scale the nodes, look at the Appearance window in the top left and click on the icon that looks like several nested circles. Click on Ranking and choose frequency as the attribute. You can set the maximum and minimum size of the nodes and for me I chose 5 and 50. Click on Apply to see what happens to the network and adjust as necessary.</p>
<p>To turn on labels, look at the very bottom of the Graph window and click on the large black T. You should now see all the labels (keywords) on the graph, but they are likely all on top of each other. They can be scaled in the same way as the nodes with most frequent appearing largest. To do this click on the icon that looks like two Ts in the Appearance window, select ranking and choose Frequency as the attribute. Again, you can decide on the minimum and maximum sizes of the labels and I settled on 0.5 and 2.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_forceatlas3.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_forceatlas3.png?raw=true" alt="Label size"></a></p>
<p>The network graph is starting to look interesting, but it needs tweaking to make it easier to read. To prevent the labels from overlapping with each other, go back to the Layout window and select the Label Adjust algorithm. Run it and watch how the network adjusts slightly so that the labels aren't on top of each other. The problem now is that everything is black, making labels difficult to read. To change this, go to the Appearance window and click on the icon that looks like a painter's palette. You should come to the Nodes &gt; Unique tabs by default with a grey colour showing. Click on apply. The nodes (dots) and edges (lines) in the network should change to grey making it much easier to see the labels.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_forceatlas4.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_forceatlas4.png?raw=true" alt="Recolour network"></a></p>
<p>At this point, you should have a usable visualisation that reveals how different keywords are clustered together and which occur most frequently. However, to make a prettier version that can be exported at high resolution, click on the Preview button at the top of the screen. Gephi will change to a different part of the program where new tools for polishing the presentation of your network are available. This part does not update automatically so everytime you want to see the effect of changes you have made, you need to click on Refresh at the bottom of the screen.</p>
<p>Under Presets in the upper left of the screen, select Default Straight and refresh the screen. You may notice that the labels are under the nodes, so select the Manage Renderers tab and move Default node labels to the top of the list.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_preview1.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_preview1.png?raw=true" alt="Renderer order"></a></p>
<p>It would be good if the labels fit better so go back to the Settings tab and uncheck Proportional size under Node Labels. Now you can change the the size of the labels by clicking on the elipses beside Font. The font size you choose will be the maximum size for the labels with the others proportionally smaller following the rule you set earlier in the Overview part of Gephi. For this example, Arial 36 Plain works well. At this size you can read all the labels and the most frequent ones are not too large.</p>
<p>When you are happy with your visualisation, you can export it by clicking on the Export: SVG/PDF/PNG button at the very bottom left of the screen.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/gephi_output.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/gephi_output.png?raw=true" alt="Gephi output"></a></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Using webscraper.io</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/scraping-webforums-with-webscraperio.html"/>
        <id>https://paddle-cluster.github.io/scraping-webforums-with-webscraperio.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T18:06:21+01:00</updated>
            <summary>
                <![CDATA[
                    To follow this tutorial, you will need have Google Chrome installed. webscraper.io is a Chrome plugin for collecting data by crawling webpages written by Mārtiņš Balodis. Go back to the Web Scraper menubar, click on 'Sitemap gardening' and choose 'Selectors' We need a way to&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>To follow this tutorial, you will need have <a href="https://www.google.com/chrome/" rel="nofollow">Google Chrome</a> installed.</p>
<p><a href="https://www.webscraper.io/" rel="nofollow">webscraper.io</a> is a Chrome plugin for collecting data by crawling webpages written by <a href="https://github.com/martinsbalodis">Mārtiņš Balodis</a>.</p>
<h2 id="mcetoc_1e14s0usq5">Setup the webscraper.io plugin</h2>
<ul>
<li>Go to <a href="https://www.webscraper.io/" rel="nofollow">webscraper.io</a> in Chrome and click on 'Download Free on Chrome Store'</li>
<li>Click on 'Add to Chrome' and follow the instructions</li>
<li>You should see a small spidersweb logo appear to the right side of the address bar</li>
<li>Click on it to see how to open 'Developer tools' where you can find webscraper.io
<ul>
<li>On Windows press Ctrl+Shift+I</li>
<li>On Mac press Cmd+Opt+I</li>
</ul>
</li>
<li>Click 'Web Scraper' in the 'Developer tools' window menu. If it does not appear in the menu, click the arrow to show more menu items</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/open_webscraper.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/open_webscraper.png?raw=true" alt="Open Web Scraper"></a></p>
<ul>
<li>If the 'Developer tools' window is on the side of the screen, follow the instructions to move it to the bottom</li>
<li>You should now see the Web Scraper window at the bottom of Chrome</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/webscraper_window.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/webscraper_window.png?raw=true" alt="Web Scraper"></a></p>
<h2 id="mcetoc_1e14s1k906">Creating a simple scraper</h2>
<ul>
<li>Click on 'Create a new sitemap' and 'Create sitemap'</li>
<li>Choose a 'Sitemap name' in this case: gardening</li>
<li>Choose a 'Start URL' in this case: <a href="https://garden.org/forums/view/gardening/" rel="nofollow">https://garden.org/forums/view/gardening/</a></li>
<li>Click on 'Create Sitemap'</li>
<li>Go to <a href="https://garden.org/forums/view/gardening/" rel="nofollow">https://garden.org/forums/view/gardening/</a> in Chrome - The forum site should appear above the Web Scraper window</li>
<li>First, lets collect the links to the different discussion threads listed
<ul>
<li>Click on 'Add new selector'
<ul>
<li>A selector is an identifiable element on the webpage from which you will collect the related content</li>
<li>'Id' is the name you will give the selector - In the end this will be the name of a data category or a column in the spreadsheet of data you produce</li>
<li>'Type' is the kind of data you want to collect from text to images, etc...</li>
<li>'Selector' is the webpage element you will use ot identify the data you want</li>
</ul>
</li>
<li>Add the 'Id' 'thread_title' and leave the 'Type' as 'Text'. We will collect the titles of all the threads listed on the page</li>
<li>Click on 'Select' under 'Selector' and run the mouse over the titles of the threads. You should see that they become highlighted in green
<ul>
<li>Click on the first one and it should turn red</li>
<li>Click on the next one and you should see that not only does it turn red, but so do the remaining thread names. The system has now recognized a common identifier for that data type and has specified it in the floating box just above the Web Scraper window
<ul>
<li>Click on 'Done selecting!'</li>
</ul>
</li>
</ul>
</li>
<li>Make sure to check the 'Multiple' box under 'Selector' so that they system collects all the instances and not just one</li>
<li>Scroll down in the Web Scraper window if needed and click on 'Save selector'</li>
</ul>
</li>
<li>Now lets collect the date and time of the last reply made in the threads
<ul>
<li>Click on 'Add new selector' and this time name it 'last_reply' and leave the 'Type' as 'Text'</li>
<li>Click on 'Select', select the boxes with the last reply times until they are all highlighted, click on 'Done selecting!', don't forget to check 'Multiple' and then 'Save selector'</li>
</ul>
</li>
</ul>
<ul>
<li>Scrape the data you have chosen to collect by going to 'Sitemap gardening' on the Web Scaper menubar and selecting 'Scrape'
<ul>
<li>You can change the speed with which the scraper crawls the website and collects data, but leave these settings alone for now and click on 'Start scraping'</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/webscraper_scraping.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/webscraper_scraping.png?raw=true" alt="Scrape"></a></p>
<ul>
<li>A table of data should appear in the Web Scraper window, but if it doesn't click 'Refresh'</li>
</ul>
<h2 id="mcetoc_1e14s2f3m7">Collecting data for the way you want it organised</h2>
<ul>
<li>
<p>Go back to the Web Scraper menubar, click on 'Sitemap gardening' and choose 'Selectors'</p>
</li>
<li>
<p>We need a way to group the data we are collecting so that each thread has its last reply date and number of replies associated with it and there is an easy way to do that</p>
<ul>
<li>Start by going back to 'Selectors' in the menubar and deleting the selectors you made</li>
<li>Click on 'Add new selector'
<ul>
<li>Give the new selector the name 'threads' and selector type 'Table'</li>
<li>For 'Selector' choose 'Select' and click within the table of threads</li>
<li>The 'Header row selector' and 'Data row selector' should automatically populate</li>
<li>Don't forget to check 'Multiple' and click 'Save selector'</li>
</ul>
</li>
<li>Click on 'Sitemap the_local', 'Scrape' to collect data again</li>
<li>A table of data should appear in the Web Scraper window, but if it doesn't click 'Refresh'</li>
</ul>
<p class="align-left"> </p>
</li>
</ul>
<h2 id="mcetoc_1e14s5p1pb">Drilling down &amp; crawling links</h2>
<ul>
<li>
<p>Go back to the Web Scraper menubar, click on 'Sitemap gardening', choose 'Selectors' and delete 'threads</p>
</li>
<li>
<p>Add a new selector of type 'Link' called 'thread' that collects the titles of each thread</p>
</li>
<li>
<p>Click on the link to a thread with replies in Chrome so that you see the posts</p>
</li>
<li>
<p>We want to group the data we collect for each post so we will use another selector type called 'Element'</p>
<ul>
<li>Create an 'Element' selector called 'post'</li>
<li>Select the outer box for 2 or 3 posts - the box containing both the poster's name and the text of their post</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/webscraper_post.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/webscraper_post.png?raw=true" alt="Selecting posts"></a></p>
<ul>
<li>Before clicking on 'Save selector', change 'Parent Selectors' to 'thread'</li>
<li>Now the scraper will run the new element selector for every link found by the 'thread' selector</li>
<li>Now we want to fill the element selector with data from each post
<ul>
<li>Click on 'thread' and on 'post'</li>
<li>Add a new selector of type 'Text' called 'poster'</li>
<li>When you click on 'Select', you should see that the 1st post is highlighted yellow. Select the name of the poster from inside that post and choose 'post' as the parent selector</li>
<li>Add a text new selector for the body text of the post</li>
</ul>
</li>
<li>Go to the Web Scraper menubar and click on 'Sitemap gardening' and choose 'Selector graph'
<ul>
<li>Click on the 'root', 'thread' and 'post' nodes to expand the graph and see how the selectors fit together. For every each post in each thread the poster and body text is collected</li>
</ul>
</li>
<li>Select 'Scrape' from the menu bar. This scrape might take a little longer. The more pages there are to crawl, the longer it takes to scrape</li>
<li>At the moment we have only scraped the 1st page of thread titles and of posts in each thread. To scrape this forum properly, we would create a link selector that crawls each page of thread tiles and then another that crawls each page of posts in a thread. We would do this using the same kind parent and child structure that we used to group the post information</li>
</ul>
<h2 id="mcetoc_1e14s67e7c">Exporting the data</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/webscraper_data.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/webscraper_data.png?raw=true" alt="Collected data"></a></p>
<ul>
<li>To export the data as a spreadsheet, go to 'Sitemap gardening' and choose 'Export data as CSV'
<ul>
<li>CSV (comma seperated values) is a standard format and will open in any spreadsheet program</li>
</ul>
</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Visualizing Twitter with TAGS</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/collecting-and-visualising-data-from-twitter-with-tags.html"/>
        <id>https://paddle-cluster.github.io/collecting-and-visualising-data-from-twitter-with-tags.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T18:05:56+01:00</updated>
            <summary>
                <![CDATA[
                     TAGS is tool for collecting and analysing Twitter built upon Google Sheets by Martin Hawksey. Look at the last row under 'Advanced Settings'. You can collect 3 types of data: Like most social media platforms, Twitter restricts the amount of data you can collect&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p> </p>
<p>TAGS is tool for collecting and analysing Twitter built upon Google Sheets by <a href="https://twitter.com/mhawksey" rel="nofollow">Martin Hawksey</a>.</p>
<ul>
<li>Go to <a href="https://tags.hawksey.info/get-tags/" rel="nofollow">TAGS</a></li>
<li>Click on TAGS v6.1</li>
<li>Make a copy of the document (It should open in Google Sheets - Login if you have to).</li>
</ul>
<h2 id="mcetoc_1e14rrv7d9">Data types</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/data_types.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/data_types.png?raw=true" alt="Data types"></a></p>
<p>Look at the last row under 'Advanced Settings'. You can collect 3 types of data:</p>
<ul>
<li>Based on search terms
<ul>
<li>'search/tweets' - Tweets from the past 7 days that match the search term you give (could be a hashtag, name, place, etc…)</li>
</ul>
</li>
<li>Based on user account
<ul>
<li>'statuses/user_timeline' - Tweets posted by a particular user</li>
<li>'favourites/list' - Tweets favorited by a particular user For each of these data types, you enter a search term in the box in the Enter term row. For 'search/tweets' you enter any text string. For 'statuses/user_timeline' and 'favourites/list', you enter an account name starting with @.</li>
</ul>
</li>
</ul>
<h2 id="mcetoc_1e14rr9ei8">Duration</h2>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/duration.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/duration.png?raw=true" alt="Duration"></a></p>
<p>Like most social media platforms, Twitter restricts the amount of data you can collect from their database at a given time.</p>
<p>Look under 'Advanced Settings' at 'Period'. Here you can choose how many days back you want to collect data for. Since Twitter caps the amount of data you can collect at one time, you may want to restrict the number of days back you collect for very active topics. The data cap is difficult to calculate, but in practice amounts to a few thousand tweets so it is rarely a problem if collecting from a specific user or for a less popular topic.</p>
<p>Look at the note at the bottom of the 'Instructions' area. You can get around Twitter’s data cap if you know ahead of time what you want to collect and setup TAGS to collect repeatedly over a period of time that could be hours or days or even years. For example, if you know that a particular course or MOOC is going to be using a a certain hashtag, you can collect tweets containing that hashtag every day for the duration of the course.</p>
<h2 id="mcetoc_1e14rqlf97">Try collecting data based on a search term</h2>
<p>Choose a term. It could be a name or a place or an event. It could be a hashtag, but doesn’t have to be.</p>
<p>But wait! How do you know what search term use?</p>
<ul>
<li>This is where sustained ethnographic engagement with the communities of interest is vital</li>
<li>The data you get is only as relevant as the selection criteria you use allows</li>
</ul>
<p>Take a look at Twitter and choose a hashtag or trend that you would like to examine. Decide on a search term.</p>
<ul>
<li>Write your chosen search term in the 'Enter term' box and make sure that 'Type' is set to 'search/tweets'</li>
<li>Go to the Google Sheets menu and choose 'TAGS' then 'Run now!'</li>
</ul>
<br><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/run_now.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/run_now.png?raw=true" alt="Run the script"></a><br>
<ul>
<li>Give TAGS permission to run the script by clicking on 'Allow'</li>
<li>Give TAGS permission to use your Twitter account to collect data
<ul>
<li>Click on 'Yes' to setup Twitter</li>
<li>Click on 'Easy Setup'</li>
<li>When the new window pops up click on 'Review Permissions' and then on 'Allow'</li>
<li>Click on 'Sign in with Twitter' and then on 'Authorise app' (Login to Twitter when prompted if needed)</li>
<li>Close the tab or window and go back to Google Sheets</li>
</ul>
</li>
<li>Go back to the menu and choose 'TAGS' then 'Run Now!' again. Wait until you see the 'Script Finished' message</li>
<li>Go to the bottom of the screen and choose the 'Archive' tab to see the data you have collected</li>
</ul>
<h2 id="mcetoc_1e14ropfj6">Ways of representing data</h2>
<p>Go back to the ‘Readme/Settings’ tab at the bottom of the screen.</p>
<p>To do this we need to open up the dataset. In the top right corner click on 'Share'</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/sharing_advanced.png?raw=true"><img style="border-style: none; max-width: 100%; background-color: #ffffff;" src="https://github.com/rocketboytom/TEfL/raw/master/sharing_advanced.png?raw=true" alt="Share&gt;Advanced"></a></p>
<ul>
<li>Click on 'Advanced' and then on ‘Change’ in the row with ‘Private - Only You can access’</li>
</ul>
<br><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/change_permissions.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/change_permissions.png?raw=true" alt="Change permissions"></a><br>
<ul>
<li>Change the setting to ‘On - Public on the web’</li>
</ul>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/sharing_on.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/sharing_on.png?raw=true" alt="Sharing on"></a><br>
<ul>
<li>Click on ‘Save’ and then on ‘Done’</li>
</ul>
<p>Under ‘Make interactive’ you will see ‘TAGSExplorer’ and ‘TAGSArchive’.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/rocketboytom/TEfL/blob/master/Visualisations.png?raw=true"><img src="https://github.com/rocketboytom/TEfL/raw/master/Visualisations.png?raw=true" alt="Visualisations"></a></p>
<ul>
<li>'TAGSExplorer' is a network graphing tool. Click on the link to it
<ul>
<li>Each circle (node) is a user who made a tweet including your search term</li>
<li>Solid lines (edges) represent one user replying to another’s tweet, the arrows show the direction</li>
<li>At the bottom right of the screen you can click on links to turn on representations of mentions and retweets, turn them on
<ul>
<li>Grey dashed lines represent mentions when one user mentions another’s name in a tweet</li>
<li>Blue dashed lines represent retweets when one user retweets (forwards) another’s tweet</li>
</ul>
</li>
<li>The size of the account name represents the number of in-links a user has in the network. In this case, in links are replies, mentions and retweets. This can be interpreted as a measure of relative importance in the network</li>
</ul>
</li>
</ul>
<ul>
<li>'TAGSArchive' visualises the dataset over time and has search tools. Go back to the main TAGS Google Sheet and click on the link to 'TAGSArchive'.
<ul>
<li>The tweets are organised chronologically</li>
<li>The timeline at the top has dots on it. Each dot represents a tweet and the visualisations shows volume of activity over time</li>
<li>Use the bars at the edges of the timeline to select specific time periods</li>
<li>The Filter boxes at the top allow you to filter tweets by the content of the tweet or the content of the user name</li>
</ul>
</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Trace ethnography</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/trace-ethnography-as-a-social-interaction-research-method.html"/>
        <id>https://paddle-cluster.github.io/trace-ethnography-as-a-social-interaction-research-method.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T18:05:27+01:00</updated>
            <summary>
                <![CDATA[
                    Trace ethnography can be used as a way to unpack sociality online. As people interact online, the traces they leave drive the digital platforms for social interaction they use. For educational researchers, traces like written documents in classrooms have always been of interest, but unlike&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Trace ethnography can be used as a way to unpack sociality online. As people interact online, the traces they leave drive the digital platforms for social interaction they use. For educational researchers, traces like written documents in classrooms have always been of interest, but unlike these traces, digital traces produced on social platforms like web forums and social media are not secondary sources for the researcher trying to understand learning in interaction. Instead, these traces are the primary interactional means and the means that users, technical platforms and researchers have for making sense of any activity that takes place. However, as social interaction online has expanded and become integral to many facets of life, the availability of trace data has exploded leaving researchers with an interest in understanding interaction on social platforms with massive datasets to handle.</p>
<h2 id="mcetoc_1e14rffuj0">Key traditions that inform trace ethnography</h2>
<ul>
<li>Documentary ethnography</li>
<li>Virtual/online ethnography</li>
<li>Computational linguistics</li>
<li>Descriptive statistics</li>
</ul>
<h2 id="mcetoc_1e14rguo28">Key concepts</h2>
<ul>
<li>
<p>Data collection</p>
<ul>
<li>API research
<ul>
<li>Using a platforms Application Programming Interface to retrieve data directly from its database</li>
<li>Most platforms provide a set of API 'hooks' - different kinds of data that can be retrieved Computational- Requests can be specified in programming languages like Python that ask a database to return specific data from specific hooks</li>
</ul>
</li>
<li>Screen scrapping
<ul>
<li>When an API is unavailable, a script can be written that behaves like a person visiting the pages of a website copying and pasting the content they want to collect into a spreadsheet</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Types of data (traces)</p>
<ul>
<li>Content
<ul>
<li>Text</li>
<li>Images</li>
<li>Audio</li>
<li>Video</li>
</ul>
</li>
<li>Meta data
<ul>
<li>Users</li>
<li>Timestamps</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Data processing</p>
<ul>
<li>Often, the data that is collected from an API research or screen scrapping process is messy and/or needs reformating for the particular needs of a study. In this sense, data must be made from the raw material that is collected online</li>
<li>Cleaning
<ul>
<li>Regular expression scripts <a href="https://regexr.com/" rel="nofollow">(REGEX)</a></li>
<li>Text parsing tools e.g. <a href="https://www.crummy.com/software/BeautifulSoup/" rel="nofollow">Beautiful Soup</a></li>
</ul>
</li>
<li>Presenting
<ul>
<li>Spreadsheets</li>
<li>Chronological threading</li>
<li>Relational and graph databases</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Analysis</p>
<ul>
<li>Computational
<ul>
<li>Exploratory data analysis
<ul>
<li>Content</li>
<li>Meta data</li>
<li>Calculated measures</li>
</ul>
</li>
<li>Natural language processing
<ul>
<li>Topic modelling</li>
<li>Concordance</li>
</ul>
</li>
<li>Social network analysis
<ul>
<li>Centrality</li>
</ul>
</li>
</ul>
</li>
<li>Ethnographic
<ul>
<li>Observation</li>
<li>Content analysis</li>
<li>Interaction analysis</li>
<li>Trace interviews</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="mcetoc_1e14rggvq7">Example workflow</h2>
<ol>
<li>Collect discussion data through platform API</li>
<li>Clean up the data and format it as a speadsheet and chronological set of discussion threads</li>
<li>Begin reading threads while trying temporal, content and actor related patterns in the data</li>
<li>Define selection criteria based on the patterns identified</li>
<li>Perform social interaction analysis on the selected threads</li>
<li>Try to unpack the broad level patterns with or in relation to the detailed interactional and technical features identified through interaction analysis</li>
<li>Perform social network analysis to identify central community members</li>
<li>Conduct trace interviews with key community members</li>
</ol>
<h2 id="mcetoc_1e14rg7iu6">General research ethics concerns</h2>
<ul>
<li>From a research ethics perspective it is important to consider what data will be collected and when/what consent is needed before starting collection
<ul>
<li>Just because a platform makes data available, doesn't mean that any use of it is ethical!</li>
</ul>
</li>
</ul>
<h2 id="mcetoc_1e14rfp9a5">Further reading</h2>
<ul>
<li><a href="https://ethnographymatters.net/blog/2016/03/23/trace-ethnography-a-retrospective/" rel="nofollow">Trace ethnography: a retrospective</a> by Stuart Geiger</li>
<li><a href="https://ethnographymatters.net/blog/2016/05/03/trace-interviews-step-by-step/" rel="nofollow">Trace Interviews Step-By-Step</a> by Elizabeth Dubois</li>
<li>Dubois, E., &amp; Ford, H. (2015). <a href="http://ijoc.org/index.php/ijoc/article/view/3378" rel="nofollow">Trace Interviews: An Actor-Centered Approach.</a> International Journal Of Communication, 9, 25.</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Packet sniffing</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/packet-sniffing.html"/>
        <id>https://paddle-cluster.github.io/packet-sniffing.html</id>
            <category term="Methods Cookbook"/>

        <updated>2020-02-15T18:06:41+01:00</updated>
            <summary>
                <![CDATA[
                    The idea with this tutorial is to give an introduction to how you can do some basic analysis of the data that is exchanged between a computer and online servers when using particular platforms or services (e.g. social media, learning management systems, etc). It discusses&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>The idea with this tutorial is to give an introduction to how you can do some basic analysis of the data that is exchanged between a computer and online servers when using particular platforms or services (e.g. social media, learning management systems, etc). It discusses some simple approaches to uncovering infrastructures that could be used to enrich ethnographic inquiries into the ways people use digital systems rather than providing a introduction to network traffic analysis for computer science purposes.</p>
<p>To follow this tutorial, you will need to have <a href="https://www.wireshark.org/" rel="nofollow">Wireshark</a> installed.</p>
<h2 id="mcetoc_1e14r11553">Collecting metadata about network traffic</h2>
<p>All traffic on data networks like the internet consists of packets that are small chunks of data. These packets come in many different forms (or protocols) and each have source and destination addresses. We can use a process called packet sniffing to collect these pieces of metadata and to get a sense of the flows of data without looking at the content of the packets themselves.</p>
<p>The first step is collect the metadata on the traffic we want to analyse. While we can filter the metadata we collect later, it is a good idea to minimize the amount of unwanted noise that is collected. For that reason, close all applications other than the one you want to collect traffic from (for this tutorial we will capture traffic from a web browser).</p>
<ul>
<li>Launch Wireshark and look at the section on the screen under the heading 'Capture'
<ul>
<li>The network interfaces you are using on your computer are listed here and you need to select one</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/wireshark_interfaces.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/wireshark_interfaces.png?raw=true" alt="Wireshark network interfaces"></a></p>
<ul>
<li>Start your web browser and watch the little heartbeat graphs beside each network interface. Double click the one that shows the most activity as you use your web browser (in my case it is 'Belkin USB-C LAN: en7)</li>
<li>You should now see a screen full of scrolling information about the packets being exchanged when you use your web browser. This information is automatically recorded by Wireshark</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/Wireshark_capture.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/Wireshark_capture.png?raw=true" alt="Wireshark capture"></a></p>
<ul>
<li>Use the platform or service that you are interested in and when you are finished, go back to Wireshark and click on the red stop button on the toolbar</li>
<li>Save your capture to file in the default 'pcapng' format</li>
</ul>
<h2 id="mcetoc_1e14r1lbc4">Locating the infrastructure</h2>
<p>Part of the pcapng file is the Internet Protocol (IP) addresses for the source and destination of each packet of data that was exchanged. Using databases that link those IP addresses to geographic locations, we can map the infrastructure that is involved while a platform or service is in use.</p>
<ul>
<li>Download the <a href="https://dev.maxmind.com/geoip/geoip2/geolite2" rel="nofollow">MaxMind GeoLite2 Country database</a> in the binary 'MaxMind DB' format
<ul>
<li>You can also download a database that resolves to the level of cities, but for this tutorial we will stick to countries</li>
</ul>
</li>
<li>In Wireshark, go to 'Edit→Preferences→Name Resolution'
<ul>
<li>Click 'Edit' next to 'Max Mind database directories'</li>
<li>Choose the folder that you put the downloaded Geolite2 database in</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/wireshark_maxmind.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/wireshark_maxmind.png?raw=true" alt="Wireshark MaxMind"></a></p>
<ul>
<li>Go to 'Statistics→Endpoints'
<ul>
<li>On the top of the new window, make sure IPv4 is selected (you can try IPv6 as well, but the newer IP standard usually has less traceable addresses)</li>
<li>At the bottom of the window, click 'Map→Open in browser'</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/wireshark_map.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/wireshark_map.png?raw=true" alt="Wireshark map"></a></p>
<h2 id="mcetoc_1e14r2esb5"><a id="user-content-revealing-hidden-infrastructure-providers" class="anchor" aria-hidden="true" href="https://github.com/paddle-cluster/constantmethod/blob/master/packets.md#revealing-hidden-infrastructure-providers"></a>Revealing hidden infrastructure providers</h2>
<p>Just as we can resolve the geographic location of the infrastructure implicated when a platform or service is used, we can also reveal the identities of the companies involved that may not necessarily be easily visible to users.</p>
<ul>
<li>Go back to the 'Statistics→Endpoints' window</li>
<li>At the bottom of the window, check the 'Name resolution' box</li>
</ul>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/constantmethod/constantmethod.github.io/blob/master/wireshark_names.png?raw=true"><img src="https://github.com/constantmethod/constantmethod.github.io/raw/master/wireshark_names.png?raw=true" alt="Wireshark map"></a></p>
<ul>
<li>If you look at the IPv4 tab, you will see the names of the servers that data were exchanged with. If you look carefully at the names, you will see that a lot of different company names are visible. Digging a little further, you will start to see patterns and note the companies with whom large numbers of packets were exchanged. This information can be visualised through a network graphing tool called <a href="https://gephi.org/" rel="nofollow">Gephi</a>, but that's a topic for another tutorial!</li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Thomas Hillman</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/thomas-hillman.html"/>
        <id>https://paddle-cluster.github.io/thomas-hillman.html</id>
            <category term="Members"/>

        <updated>2020-02-15T19:08:46+01:00</updated>
            <summary>
                <![CDATA[
                    Thomas Hillman is conveynor of PADDLE. His work goes over the boundaries between the fields of Informatics, Learning Sciences, Human-computer Interaction, and Science and Technology Studies. He works at the Department of Applied IT, is Associate Professor of Education and has a background in the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Thomas Hillman is conveynor of PADDLE. His work goes over the boundaries between the fields of Informatics, Learning Sciences, Human-computer Interaction, and Science and Technology Studies. He works at the Department of Applied IT, is Associate Professor of Education and has a background in the design of technologies for learning. In his research, Thomas examines the ways that different technologies shape learning as an individual, social and cultural process. This work is relevant for understanding how we come to know in relation to technologies, how we might best design or adapt technologies to support learning, and how we might work with technologies as part of teaching and learning practices.</p>
<p>In recent years, Thomas' work has focused on the blurring boundaries between online and offline activities in many aspects of contemporary life and in relation to diverse learning settings. Much of his research relies on extensive use of video-recordings and digital records of the ways people interact with technologies and Thomas works to adapt and develop methods and tools for gaining access to and making sense of these activities.</p>
<p>Current research projects:</p>
<ul>
<ul>
<li><a href="https://paddle-cluster.github.io/red.html">Reconfigurations of educational in/equality in a digital world (RED)</a> - <em>Financed by Volkswagen Foundation, Riksbankens Jubileumsfond, Novo Nordisk Foundation, &amp; Compagnia di San Paolo</em><br> </li>
</ul>
</ul>
<a href="https://paddle-cluster.github.io/socdex.html"> Social dimensions of expertise development in networked communities (SOCDEX) </a>
<ul>
<li> - <em>Financed by the Swedish Research Council</em></li>
</ul>
<ul>
<li><a href="https://wasp-hs.org/projects/professional-trust-and-autonomous-systems/">Professional trust and autonomous systems</a> - <em>Financed by the Wallenberg AI, Autonomous Systems and Software Program - Humanities and Society (WASP-HS)</em></li>
</ul>
<ul>
<li><a href="https://paddle-cluster.github.io/balanced.html">Teachers' digital work - (in)balance between demands and support? (BalancED)</a> - <em>Financed by Forte</em></li>
</ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>RED</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/red.html"/>
        <id>https://paddle-cluster.github.io/red.html</id>
            <category term="Projects"/>

        <updated>2020-02-15T11:34:21+01:00</updated>
            <summary></summary>
        <content></content>
    </entry>
    <entry>
        <title>BalancED</title>
        <author>
            <name>Thomas Hillman</name>
        </author>
        <link href="https://paddle-cluster.github.io/balanced.html"/>
        <id>https://paddle-cluster.github.io/balanced.html</id>
            <category term="Projects"/>

        <updated>2020-02-15T11:34:30+01:00</updated>
            <summary></summary>
        <content></content>
    </entry>
</feed>
